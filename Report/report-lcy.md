### LGB

##### 模型简介

LightGBM是个快速的，分布式的，高性能的基于决策树算法的梯度提升框架。可用于排序，分类，回归以及很多其他的机器学习任务中。传统的boosting算法，需要对每一个特征扫描所有的样本点来选择最好的切分点，这是非常耗时的，为了解决这种在大样本高维度数据环境下耗时的问题，Lightgbm主要使用了两个算法：

- Gradient-based One-Side Sampling(GOSS)，  基于梯度的单边采样，主要思想不是使用所有的样本点计算梯度，而是对样本进行采样来计算梯度。GOSS保留所有的梯度较大的实例，在梯度小的实例上使用随机采样。为了抵消对数据分布的影响，计算信息增益的时候，GOSS对小梯度的数据引入常量乘数。GOSS首先根据数据的梯度绝对值排序，选取top a个实例。然后在剩余的数据中随机采样b个实例。接着计算信息增益时为采样出的小梯度数据乘以(1-a)/b，这样算法就会更关注训练不足的实例，而不会过多改变原数据集的分布。

- Exclusive Feature Bundling（EFB），互斥特征绑定，在高维度的数据中很多特征都是互斥的，也就是他们很少同时取非零值，这使得我们可以将很多特征绑定在一起从而形成一个特征，减少特征的维度，使寻找最佳切分点的消耗减少lightgbm同样采用特征抽样，但是选择将互斥的特征绑定在一起从而减少特征的维度。由于将特征拆成更小的互斥绑定数量，是NPhard的问题，所以这里放松了条件允许存在少数的样本点特征并不是互斥的。EFB算法首先构造一个边带有权重的图，其权值对应于特征之间的总冲突，然后通过特征在图中的度来降序排序特征，最后检查有序列表中的每个特征，并将其分配给具有小冲突的现有bundling，或创建新bundling。

结合使用GOSS和EFB的GBDT算法就是LightGBM。同时LigntGBM对上面算法进行了其他优化，大致有以下几点

- 直方图优化

- 存储记忆优化

- 深度限制的节点展开方法

- 直方图做差优化

- 顺序访问梯度

- 支持类别特征

- 支持并行学习

  

因为很多开源的代码都使用了lightGBM的模型，所以这一部分主要是参考了一些开源的代码，分别学习了其中的一些思路和方法，主要参考的开源代码见参考资料。



##### 数据预处理

1. 合并导入数据集

   合并历史销量数据、车型搜索数据、新闻评论数据和车型评论数据以及测试集。合并测试集是为了方便后面处理中对测试集特征的填充。

   

2. 异常值的发现和处理和缺失数据的填充

   这里将位于四分位距1.5倍以外的数据视为异常值，主要处理的数据是搜索数据、新闻评论数和评论数据

   使用均值填充异常值

   

3. 填充测试集缺失的数据

   根据三年数据的比例填写2018年待遇测数据除销量之外的其他特征

   ```python
   # 填充测试集数据
   for col in ['carCommentVolum','newsReplyVolum','popularity','bt_ry_mean','ad_ry_mean', 'md_ry_mean','bt_ry_rm_sum','ad_ry_rm_sum','md_ry_rm_sum']:
       lgb_col_na = pd.isnull(data[col])
       data[col] = data[col].replace(0,1)
       data.loc[pd.isnull(data[col]),col] = ((((data.loc[(data['regYear'].isin([2017]))&(data['regMonth'].isin([1,2,3,4])), col].values /
       data.loc[(data['regYear'].isin([2016]))&(data['regMonth'].isin([1,2,3,4])), col].values)))*
       data.loc[(data['regYear'].isin([2017]))&(data['regMonth'].isin([1,2,3,4])), col].values * 1.03).round()
   ```

   

4. 非数值特征编码

   编码非数值特征，主要是车身类型和车型



##### 特征工程

原有数据集中的数据包括：

省份，车型，车身类型，年，月，销量，搜索量，对车型相关新闻文章的评论数量，对车型的评价数量

1. 构造特征

   - 更改月份值，由于月份和年表示，并且增加了一个特征表示这是2016年起的第几个月，方便后面下移特征

   - 我首先想到的添加的特征是能够表示总体的特征,添加了**每年每个车身类型的总销量、平均销量，每年每个省份的总销量、平均销量，每年每个车型的总销量、总体销量**，根据经验，我认为车型、车身类型和省份销量的占比也可以作为特征，*显然这些特征是冗余的，经过后面实验最终只保留三个总销量*

   - 通过可视化销量和其他数据，可以发现这些数据与时间高度相关，而且根据对相关资料的查询发现一般而言，每年春节左右乘用车销量会有很大的涨幅，所以添加了特征 aroundNY表示在春节左右，这里取了每年春节前后的三个月，同时根据月份添加了权重值，将1、2、3、4的权重设为6，其他月份的权重设为4

     

2. 使用历史销量作为特征

   - 由于是预测模型，使用历史销量和其他数据作为特征，首先尝试将12个月前的数据下移作为本月的特征，但是后面发现这样效果不是特别好

   - 将4个月前的销量，搜索量，对车型相关新闻文章的评论数量，对车型的评价数量，三个总体销量下移作为特征

     

3. 填充缺失的特征值

   使用历史销量作为特征的问题是开始的4或12个月没有数据可以下移，需要自行填写。这里仍然按照比值相对固定的原则，即2017年数据和2016年数据的比值应该与2016年数据比2015年数据基本类似，这里需要填写的即为2015年的数据。

   

4. 最终使用的特征包括：前4和12个月的所有下移特征，省份，车身类型，车型，年份，月份，是否在新年附近，月份权重，同时每次加入销量均值最后取平均



##### 结果分析

训练集：距离2016年小于等于20个月的数据

验证集：21到24个月的数据

测试集：2018年1到4月的数据



- 开始因为对LightGBM模型没有理解，并且在测试集中填充的特征不合理，结果只有0.4左右。

- 在可视化数据之后可以发现销量明显和销售的月份是相关的，而且相关资料确实有类似的结论，所以直接把特征下移了12个月，但是效果很不好。

- 重新填充了测试集和特征，改为使用4个月前的特征下移结果比下移12个月好很多。

  原因可能是提供的数据量比较少，如果直接下移12个月的话，相当于没有使用2017年的5-12月数据，并且回填的位于2016年的起始数据可能都会有很大的误差。

- 最终把下移12个月和下移4个月都当做特征训练，结果输出特征的重要程度发现，除了车型、省份、月份这些特征外，前4个月的销量和其他特征总体来说比前12个月下移的特征重要性更高。按照常识加入的一些关于时间的特征，比如在表示在新年附近的特征和月份的权重重要性很低。
- 一个重要性输出如下：

```
lgb Features: 
[('model', 13562), 
('adcode', 10534), 
('shift_model_adcode_mt_label_4', 6451),
('regMonth', 6026), 
('shift_model_adcode_mt_label_12', 5048), 
('shift_model_adcode_mt_popularity_4', 4742), 
('shift_model_adcode_mt_ad_ry_rm_sum_4', 3947), 
('shift_model_adcode_mt_popularity_12', 3462),
('shift_model_adcode_mt_ad_ry_rm_sum_12', 3119), 
('shift_model_adcode_mt_md_ry_rm_sum_4', 3050), 
('shift_model_adcode_mt_newsReplyVolum_4', 2860), 
('shift_model_adcode_mt_carCommentVolum_4', 2823), 
('shift_model_adcode_mt_newsReplyVolum_12', 2521), 
('shift_model_adcode_mt_carCommentVolum_12', 2497), 
('ad_ry_mean', 2344), 
('shift_model_adcode_mt_md_ry_rm_sum_12', 2229), 
('shift_model_adcode_mt_bt_ry_rm_sum_4', 2037), 
('shift_model_adcode_mt_bt_ry_rm_sum_12', 1507),
('bodyType', 556), 
('aroundNY', 226), 
('weightMonth', 211), 
('regYear', 3)]
```

总的来说，因为对模型不是非常了解，基本没有进行调参的工作，主要是进行的是特征和数据处理的工作。但是感觉最后的结果总是和常识有一些出入，可能是因为对数据的处理仍然存在一些问题，最终结果在0.55左右，并没有发挥出传说中LightGBM可以达到的很好的效果。



##### 参考资料

https://www.biaodianfu.com/lightgbm.html

https://blog.csdn.net/qq_24519677/article/details/82811215

开源LGB baseline参考

 https://zhuanlan.zhihu.com/p/82738239 

 https://zhuanlan.zhihu.com/p/82416104 