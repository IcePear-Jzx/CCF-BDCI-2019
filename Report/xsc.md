## 另一种全连接网络

考虑到有很多数据都是Category的, 我一开始是想训练一个end-to-end的模型, 输入年份, 月份, 地区, 型号等数据, 然后输出预测的销量. 我想先尝试一下全连接网络.

首先我考虑, 对这些Category的数据, 我是直接把one-hot向量送进模型, 还是加一层embedding呢? 我决定都尝试一下.  我先试了直接使用one-hot向量的办法. 

说实话这种办法有些离经叛道, 因为我似乎送来没有见过直接把one-hot向量直接加进全连接层的. 这样做看起来很没有道理. 但我懒得管这么多了, 因为做一次这种尝试的成本是很低廉的.

在这一版的全连接网络里, 输入由Category的数据和Numerical的数据组成.考虑输入
$$
x = \{x_0, \dots, x_l, x_{l+1}, \dots, x_n\}
$$
其中$x_0, \dots, x_l$为one-hot之后的Category数据, 比如说, $x_0, x_1, \dots, x_{11}$代表了一项销售数据的月份, 假如这个销售数据产生在1月, 那么有
$$
x_0 = 1, \\x_1, \dots, x_{11} = 0
$$
 $x_{l+1}, \dots, x_n$为Numerical的数据, 也就是说
$$
x_0, \dots, x_l \in \{0, 1\}\\
x_{l+1}, \dots, x_n \in \mathscr{R}
$$
我把这两种数据一视同仁地视为全连接网络的输入, 也就是有
$$
h_1 = \sigma_0(W_0x+b_0)\\
\dots\\
h_k = \sigma_{k-1}(W_{k-1}h_{k-1} + b_{k-1})\\
\dots\\
y = \sigma_{n-1}(W_{n-1}h_{n-1} + b_{n-1})
$$
当然, 其实这种做法其实是相当悄然地于做了embedding的. embedding出现在哪里呢? 就在一开始.
$$
h_1 = \sigma_0(W_0x+b_0)
$$
目录型数据的embedding其实就隐藏在$W_0$里, 这是不难发现的. 无论如何, 现在开始考虑训练的过程和一些trick.

对于训练的过程， 我们输入的当然是需要预测的细分市场的信息， 包括月份， 省份，车型这三个最重要的Category数据. 此外, 还可以输入对应的省份, 车型和月份的评论数量.  Category数据embedding之后获得的0-1向量和numerical数据进行拼接, 得到了模型的输入向量.  模型的输出值当然是一个表示2019年销量预测值的数值.



##一维CNN

### 思路

为什么会想到使用CNN呢? 这基于对销售数据的一个重要的观察, 或者说假设.  那就是, **对于特定的省份和车型, 它在2017年的某月的销量, 和它在2016年的这个月及其附近几个月的销量关系是最大的.**

例如, 我们想要靠2016年的销量数据来预测特定车型在特定省份在2017年5月的销量. 我们应该怎么做呢? 假如我们只能从2016年12个月的销售数据中选取一个月的数据来预测2017年5月的销售数据, 那么我们一定会选择通过2016年5月的销量数据, 因为2016年5月的销量数据对2017年5月的销量影响是最大的. 那么如果只能只能从2016年12个月的销售数据中选取两个月的数据来预测2017年5月的销售数据, 那么我们一定会选择5月和4月, 或者5月和6月.

这在直观上是易于理解的. 不同的年份中相同的月份, 有着类似的特点, 如气候, 经济形势, 节假日情况, 甚至消费潮流.  为了预测2017年5月的销量, 选取2016年5月及其附近的月份的数据是合适的.

那么, 也许使用2016年全年的销量数据来预测2017年5月的销量数据的效果是最好的? 其实未必. 因为, 2016年1月或者2016年12月的销量数据, 和2017年5月的销量其实关系已经不是很大了. 甚至, 它们可能不仅关系不大, 还可能对预测起到负面效果.

 之前提到的MLP模型使用了某一年全年的数据来预测下一年某个月的销售数据, 但是在这个模型中, 为了预测2018年5月的销售额, 2017年5月, 4月, 以及12月的销量以相同的权重进入模型. 显然这些数据的权重是不同的. 虽然在模型的第一层, 可以训练得到这些数据的不同权重, 但训练毕竟有时候是玄学的, 因为种种原因可能并不能获得最优解.

所以, 我们希望能够经过人工的方法, 使得在预测5月分的数据时, 模型能够专注于3,4,5,6几个月的数据. 用attention机制当然可以实现这个目的, 但在这之前我们先尝试另外一个方法, 那就是一维CNN.

### 过程

在训练阶段, 对于每一个需要预测的月份, 我们使用一个一维卷积核. 这个卷积核的大小一般是3或5. 为了预测4月份的销量, 使用一个一维卷积核来提取去年3-5月的销量, 也许在经过一些变换, 从而得到输出的结果.

在训练过程中, 2016年的数据是训练数据, 而2017年的数据是ground truth. 当然, 在预测过程中, 2017年的数据被用于预测, 而输出的是2018年的数据.

### 结果

事实证明这是一个非常不错的想法. 在未经任何有意识调参的情况下, 也就是说, 随手一跑, 得分就达到了0.5左右. 有理由相信经过一段时间的调参时候, 这个模型的得分会很不错.

**当然, 我不想调. 调参是缺乏意义的机械劳动.**

## LSTM

### 思路

**一个月接着一个月的销量, 销量预测怎么少得了我RNN.**

> 时间序列预测问题是一个复杂的预测模型问题，它不像一般的回归预测模型。时间序列预测的输入变量是一组按时间顺序的数字序列。它既具有延续性又具有随机性，所以在建模难度上相对回归预测更大。但同时，正好有一种强大的神经网络适合处理这种存在依赖关系的序列问题：RNN（Recurrent neural networks）。在过去几年中，应用 RNN 在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且应用领域还在扩展。

**既然是RNN, 怎么少得了LSTM.**

用LSTM来做销量预测应该是一个非常烂大街的做法了. 但是考虑到**这个赛题的数据量比较少, 在时序预测方面强大无比的LSTM可能并不能发挥多大的性能.** 看一些选手关于这个赛题的讨论和分享, 基本上没有人用LSTM, (甚至基本上没有人用深度学习). 但尝试一下总是合理的.

### 过程

把对于特性的型号和省份, 把每个月的销量, 搜索度, 评论数进行归一化之后拼成一个向量, 再把每个月的这个向量看做一个输入序列. 这样加入一层或两层LSTM之后再送进一些全连接网络. 

用四个月或者以上的数据来预测下一个月的数据.

### 效果

中规中矩, 勉勉强强. 0.4-0.5之间的得分. 应该是归功于贫乏的数据量, 本来在时序预测领域大放异彩的LSTM在这个赛题下只能当个弟弟. 当然, 数据的方差过大, 没有做平稳化也是重要的原因.

## (D)FM

这是一次放飞自我的尝试. 推荐系统领域的人对如雷贯耳的FM肯定不陌生. 这个模型对推荐系统中的一些Category的特征处理有极佳的效果. 它及其变种可能是工业界最流行的推荐系统模型了(当然在学术界可能并非如此).

为啥想到用这玩意呢? 因为在这个赛题里就有很多Category的数据, 车型, 省份, 车身种类是Category的, 月份和年份也可以看成是Category的. 只有销量, 评论数和搜索数等数据不是Category的.  我寻思这个模型效果应该挺好.

你可能要说了.  **是的, FM模型不能处理时序信息, 因为根本就没有任何关于"2018"这个标签的数据. 这是我一开始没注意的.** 因为我一开始没仔细读题目, 以为是从一段时间里均匀地挖出一些空, 让我们预测这些空里的销量.  我按这种方式划分了训练集和测试集之后, 效果确实非常好. 但实际上赛题是给2016和2017的数据让预测2018的数据. 等我写完这个模型之后才发现这一点(反正也只花了一两小时), 这就尴尬了.

不过我还是想把这个尝试写出来, 因为这个模型确实优雅又强大.

> ​    因子分解机(Factorization Machine, FM)是由Steffen Rendle提出的一种基于矩阵分解的机器学习算法。对于因子分解机FM来说，最大的特点是对于稀疏的数据具有很好的学习能力。现实中稀疏的数据很多，例如作者所举的推荐系统的例子便是一个很直观的具有稀疏特点的例子。

考虑一个特征$x=\{x_1, ..., x_n\}$, FM的计算模型为
$$
\hat{y} = w_0 + \sum_{i=1}^{n-1} w_i x_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}<v_i, v_j>x_ix_j
$$
其中, 参数$w_0 \in R$, $w \in R, V \in R^{n\times k}$. $<v_i, v_j>$表示内积.

$v_i$表示系数矩阵$V$的第i维向量 .在因子分解机FM模型中，前面两部分是传统的线性模型，最后一部分将两个互异特征分量之间的相互关系考虑进来。因子分解机FM也可以推广到高阶的形式，即将更多互异特征分量之间的相互关系考虑进来。

在此基础上做一个Deepen的推广, 就是DFM了.

编写代码实现FM和DFM模型. 之后, 用省份, 车型等信息的one-hot向量作为输入. 模型的效果和训练的方式有关. 如果在一段时间里均匀地挖出一些空, 这个模型能很好地预测这些空里的销量. 但是用一段时间取预测下一段时间, 这个模型的效果并不好. 动手做之前还是得好好审题啊.

## SVR

这么经典的方法当然少不了啦.

SVR全称是support vector regression，是SVM（支持向量机support vector machine）对回归问题的一种运用。SVM与logistic分类器类似，也是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。

我们知道，最简单的线性回归模型是要找出一条曲线使得残差最小。同样的，SVR也是要找出一个超平面，使得所有数据到这个超平面的距离最小。

对于一般的回归问题，给定训练样本D={(x1,y1),(x2,y2),...,(xn,yn)},yi€R,我们希望学习到一个f（x）使得其与y尽可能的接近，w，b是待确定的参数。在这个模型中，只有当f(x)与y完全相同时，损失才为零，而支持向量回归假设我们能容忍的f(x)与y之间最多有ε的偏差，当且仅当f(x)与y的差别绝对值大于ε时，才计算损失，此时相当于以f(x)为中心，构建一个宽度为2ε的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。（间隔带两侧的松弛程度可有所不同）.

前面说了，SVR是SVM的一种运用，基本的思路是一致，除了一些细微的区别。使用SVR作回归分析，与SVM一样，我们需要找到一个超平面，不同的是：在SVM中我们要找出一个间隔（gap）最大的超平面，而在SVR，我们定义一个ε，如上图所示，定义虚线内区域的数据点的残差为0，而虚线区域外的数据点（支持向量）到虚线的边界的距离为残差（ζ）。与线性模型类似，我们希望这些残差（ζ）最小。所以大致上来说，SVR就是要找出一个最佳的条状区域（2ε宽度），再对区域外的点进行回归。对于非线性的模型，与SVM一样使用核函数（kernel function）映射到特征空间，然后再进行回归。

因此,SVR的问题可转化为
$$
min_{w,b} \frac{1}{2}||w||^2 + C\sum_{i=1}^ml_c(f(x_i) - y_i)
$$
$l$为损失函数
$$
l_{\epsilon}(z) = 0, if\ |z| \le \epsilon; |z| - \epsilon,\ else
$$
因此引入松弛因子, 重写第一个式子为
$$
min_{w,b,\zeta_i, \hat{\zeta}_i} \frac{1}{2}||w||^2+C\sum_{i=1}^m(\zeta_i+\hat{\zeta}_i)\\
s.t.\ f(x_i) - y_i \le \epsilon + \zeta_i\\
y_i - f(x_i) \le \epsilon + \hat{\zeta_i}\\
\zeta_i \ge 0, \hat{zeta_i} \ge 0, i = 1,2,..,m
$$
最后, 用优化方法对上式求解即可. (我这里用tensorflow梯度下降直接求了, 其实用更加数学的方法.)

用前几个月的销量做输入, 下一个月的销量做输出, 对模型进行训练. 很轻易地达到了0.5以上的分数.